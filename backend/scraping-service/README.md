# Express Scraper API

üöÄ **Serveur Express de scraping dynamique** avec configuration JSON personnalisable.

## ‚ú® Fonctionnalit√©s

- ‚úÖ **Scraping dynamique** avec configuration JSON
- üîó **Suivi de liens** configurable
- üìÑ **Pagination automatique**
- üéØ **Extraction de champs** flexibles (texte et attributs)
- üõ°Ô∏è **Validation** robuste des configurations
- ‚ö° **Performance** optimis√©e avec Puppeteer
- üîí **S√©curit√©** avec helmet et validation

## üõ†Ô∏è Installation

```bash
# Installer les d√©pendances
npm install

# D√©marrer le serveur
npm start

# Mode d√©veloppement (avec nodemon)
npm run dev
```

## üì° API Endpoints

### `POST /api/scrape`

Effectue un scraping selon la configuration fournie.

#### Configuration JSON

```json
{
  "startUrl": "https://example.com/products",
  "followLinks": {
    "selector": "a[href*='/questions/']",
    "limit": 10
  },
  "scrapeFields": {
    "title": "h1.product-title",
    "price": ".price-value",
    "description": ".product-description p",
    "image": "img.product-image@src"
  },
  "nextPageSelector": ".pagination-next",
  "maxPages": 5
}
```

#### Param√®tres

| Param√®tre              | Type   | Requis | Description                             |
| ---------------------- | ------ | ------ | --------------------------------------- |
| `startUrl`             | string | ‚úÖ     | URL de d√©part pour le scraping          |
| `followLinks`          | object | ‚ùå     | Configuration pour suivre les liens     |
| `followLinks.selector` | string | ‚úÖ     | S√©lecteur CSS pour les liens √† suivre   |
| `followLinks.limit`    | number | ‚ùå     | Nombre max de liens (d√©faut: 10)        |
| `scrapeFields`         | object | ‚úÖ     | Champs √† extraire avec leurs s√©lecteurs |
| `nextPageSelector`     | string | ‚ùå     | S√©lecteur pour le bouton de pagination  |
| `maxPages`             | number | ‚ùå     | Nombre max de pages (d√©faut: 5)         |

#### Extraction de champs

- **Texte** : `"selector": "h1.title"`
- **Attribut** : `"selector": "img.photo@src"`
- **Plusieurs √©l√©ments** : automatiquement d√©tect√© et retourn√© comme tableau

#### S√©lecteurs de pagination

- **S√©lecteur CSS** : `"nextPageSelector": ".pagination-next"`
- **Par texte** : `"nextPageSelector": "text:Next"`
- **Par attribut** : `"nextPageSelector": "a[rel='next']"`

#### R√©ponse

```json
{
  "success": true,
  "startUrl": "https://example.com/products",
  "totalResults": 25,
  "results": [
    {
      "url": "https://example.com/product/1",
      "timestamp": "2024-01-15T10:30:00.000Z",
      "title": "Produit 1",
      "price": "29.99‚Ç¨",
      "description": "Description du produit...",
      "image": "https://example.com/image1.jpg"
    }
  ],
  "timestamp": "2024-01-15T10:30:00.000Z"
}
```

### `GET /api/example-config`

Retourne un exemple de configuration avec documentation.

### `GET /health`

V√©rification de sant√© du serveur.

## üéØ Exemples d'utilisation

### Scraping simple (page unique)

```bash
curl -X POST http://localhost:3000/api/scrape \
  -H "Content-Type: application/json" \
  -d '{
    "startUrl": "https://example.com",
    "scrapeFields": {
      "title": "h1",
      "content": "p"
    }
  }'
```

### Scraping avec suivi de liens

```bash
curl -X POST http://localhost:3000/api/scrape \
  -H "Content-Type: application/json" \
  -d '{
    "startUrl": "https://example.com/products",
    "followLinks": {
      "selector": "a.product-link",
      "limit": 5
    },
    "scrapeFields": {
      "name": "h1.product-name",
      "price": ".price",
      "image": "img.main-image@src"
    }
  }'
```

### Scraping avec pagination

```bash
curl -X POST http://localhost:3000/api/scrape \
  -H "Content-Type: application/json" \
  -d '{
    "startUrl": "https://example.com/articles",
    "scrapeFields": {
      "title": "h2.article-title",
      "date": ".publish-date"
    },
    "nextPageSelector": ".next-page",
    "maxPages": 3
  }'
```

### Scraping avec pagination par texte

```bash
curl -X POST http://localhost:3000/api/scrape \
  -H "Content-Type: application/json" \
  -d '{
    "startUrl": "https://stackoverflow.com/questions",
    "scrapeFields": {
      "title": "h3 a",
      "votes": ".s-post-summary--stats-item:first-child"
    },
    "nextPageSelector": "text:Next",
    "maxPages": 3
  }'
```

### Exemple complet (e-commerce)

```bash
curl -X POST http://localhost:3000/api/scrape \
  -H "Content-Type: application/json" \
  -d '{
    "startUrl": "https://example.com/category/electronics",
    "followLinks": {
      "selector": "a[href*=\"/product/\"]",
      "limit": 10
    },
    "scrapeFields": {
      "title": "h1.product-title",
      "price": ".price-current",
      "originalPrice": ".price-original",
      "rating": ".rating-score",
      "reviews": ".review-count",
      "image": "img.product-image@src",
      "description": ".product-description p"
    },
    "nextPageSelector": ".pagination-next",
    "maxPages": 3,
  }'
```

## üîß Comportement du scraper

### Algorithme de scraping

1. **Initialisation** : Lancement du navigateur Puppeteer
2. **Page de d√©part** : Navigation vers `startUrl`
3. **Pour chaque page** :
   - Si `followLinks` est d√©fini :
     - R√©cup√©ration des liens selon le s√©lecteur
     - Visite de chaque lien
     - Extraction des donn√©es sur chaque page de lien
     - Retour √† la page principale
   - Sinon : extraction directe sur la page actuelle
4. **Pagination** :
   - V√©rification du bouton `nextPageSelector`
   - Clic et navigation vers la page suivante
   - R√©p√©tition jusqu'√† `maxPages` ou absence de bouton
5. **Finalisation** : Fermeture du navigateur et retour des r√©sultats

### Gestion des erreurs

- **Liens bris√©s** : Ignor√©s avec log d'erreur
- **S√©lecteurs invalides** : Champs vides retourn√©s
- **Pagination** : Arr√™t gracieux si bouton indisponible

## üîí S√©curit√©

- Validation stricte des configurations
- Blocage des ressources non n√©cessaires
- Headers de s√©curit√© avec Helmet
- Pas d'ex√©cution de JavaScript c√¥t√© client

## üêõ D√©pannage

### Erreur de lancement Puppeteer

```bash
# Sur Linux, installer les d√©pendances
sudo apt-get install -y chromium-browser

# Sur macOS avec Homebrew
brew install chromium
```

### Probl√®mes de performance

- R√©duire `maxPages` et `followLinks.limit`
- Utiliser des s√©lecteurs plus sp√©cifiques

### Debugging

```bash
# Logs d√©taill√©s
DEBUG=puppeteer:* npm start

# Mode d√©veloppement
npm run dev
```
